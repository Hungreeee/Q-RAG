{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-form QnA\n",
    "\n",
    "In this notebook, we aim to construct and evaluate the performance of a Q-RAG agent. While in a realistic use case, it would have require developers to set up graph-like databases and the core elements of Q-RAG (e.g., question-chunk connections), this notebook show that Q-RAG can be easily replicated using a simple vectorstore and document metadata. Here, we would like to utilize LangChain - a library providing wrappers for framework that are very easy and quick to set up - to build our agents.\n",
    "\n",
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important technical details before we start:\n",
    "\n",
    "- LLM: `gpt-4o-mini`\n",
    "- Vectorstore: `FAISS`\n",
    "- Embedding Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Evaluation Dataset: `rajpurkar/squad` (HuggingFace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "DATASET = \"rajpurkar/squad\"\n",
    "\n",
    "FAISS_PATH = \"./vectorstore/squad\"\n",
    "RESULTS_PATH = \"./results/squad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model\n",
    "\n",
    "We would like to load the embedding model `sentence-transformers/all-MiniLM-L6-v2`, a good option due to its light-weight and fast inference ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL, \n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we take a sample of 1000 points from the validation split of the dataset for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full dataset\n",
    "full_dataset = load_dataset(DATASET, split=\"validation\")\n",
    "\n",
    "# Sample 1000 datapoints from the full dataset\n",
    "random.seed(13)\n",
    "rand_indices = random.sample(range(0, len(full_dataset)), 1000)\n",
    "dataset = full_dataset.select(rand_indices)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the existing `context` chunks in the (full) dataset are grouped together to form a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2067 entries, 0 to 10565\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      2067 non-null   object\n",
      " 1   title   2067 non-null   object\n",
      " 2   text    2067 non-null   object\n",
      " 3   type    2067 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Construct the corpus of context chunks\n",
    "document_df = pd.DataFrame({\n",
    "    \"id\": full_dataset[\"id\"],\n",
    "    \"title\": full_dataset[\"title\"],\n",
    "    \"text\": full_dataset[\"context\"],\n",
    "    \"type\": \"chunk\"\n",
    "})\n",
    "\n",
    "document_df = document_df.drop_duplicates([\"text\"])\n",
    "document_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunks are then further splited into smaller chunks (size of around 300 chars) for better retrieval. This chunk corpus will then be loaded into a FAISS vectorstore instance, which are stored locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform text chunking\n",
    "loader = DataFrameLoader(document_df, page_content_column=\"text\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "document_chunks = loader.load()\n",
    "document_chunks = text_splitter.split_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text chunks into a FAISS vectorstore instance\n",
    "vectorstore_db = FAISS.from_documents(\n",
    "    documents=document_chunks, \n",
    "    embedding=embedding_model, \n",
    "    normalize_L2=True\n",
    ")\n",
    "\n",
    "# Save the vectorstore locally\n",
    "vectorstore_db.save_local(f\"{FAISS_PATH}/chunk-vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to initiate our vectorstore retrievers. For this design, let us have two retrievers serving different purposes:\n",
    "\n",
    "- Chunk vectorstore: This will be used to obtain relevant chunks given a query as normally. We only need to load the indexed vectors from our local storage.\n",
    "- Question vectorstores: This will be used to retrieve similar \"model questions\". We will need to create a new instance of FAISS vectorstore, with a default \"mock question\" inside (simply because it is not possible to create an empty database).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../images/q-rag-idea.png\" width=60%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload chunk vectorstore from local\n",
    "chunks_vectorstore_db = FAISS.load_local(\n",
    "    folder_path=f\"{FAISS_PATH}/chunk-vectorstore\", \n",
    "    embeddings=embedding_model, \n",
    "    normalize_L2=True,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model question will have a list-type attribute called `connections`, which later can be used to store IDs of relevant chunks. This helps to replicate the model question-chunk connections in Q-RAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a placeholder question vectorstore \n",
    "question_vectorstore_db = FAISS.from_documents(\n",
    "    documents=[\n",
    "        Document(page_content=\"Mock Question\", metadata={\n",
    "            \"id\": 0, \n",
    "            \"type\": \"question\", \n",
    "            \"connections\": []\n",
    "        })],\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the OpenAI's `gpt-4o-mini` as our default LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiaate LLM Agent\n",
    "llm_agent = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_MODEL,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RAG Agent class template is created here, unifying the retrievers and the generator into one pipeline. This agent has the following functionality:\n",
    "\n",
    "- Utilize the chunk database / question database to perform retrieval. If one recalls, besides the generic chunk retrieval process, Q-RAG also attempts to retrieve relevant questions to obtain the connected context chunks. \n",
    "- Construct the context string from the raw retrieval results to augment the input user query. The complete prompt will then be processed by LLM generator to obtain answers.\n",
    "\n",
    "As a side note, the SQuAD dataset focuses on short-form question-answering, meaning that the answers will be very short and concise. Here, we engineer our prompt template to have the same behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: ChatOpenAI,\n",
    "        chunk_retriever: FAISS,\n",
    "        question_retriever: FAISS,\n",
    "        dataframe: pd.DataFrame\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.client = client\n",
    "        self.chunk_retriever = chunk_retriever\n",
    "        self.question_retriever = question_retriever\n",
    "\n",
    "    \n",
    "    def retrieve_context(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int,\n",
    "        distance_threshold: float,\n",
    "        retrieve_questions: bool = False,\n",
    "        **krawgs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks/questions given query\n",
    "        \"\"\"\n",
    "        # Adjust retrieval to be chunk-based or question-based\n",
    "        retriever = self.chunk_retriever if not retrieve_questions else self.question_retriever\n",
    "        \n",
    "        # Perform similarity search for relevant chunks / questions\n",
    "        docs_with_metadata = retriever.similarity_search_with_score(\n",
    "            query=query,\n",
    "            k=top_k,\n",
    "            **krawgs\n",
    "        )\n",
    "        # Filter chunks / questions within the specified threshold\n",
    "        filtered_docs = [doc for doc, score in docs_with_metadata if score <= distance_threshold] \n",
    "        \n",
    "        if retrieve_questions:\n",
    "            # Obtain connected document IDs for retrieved questions\n",
    "            doc_ids = [doc for relevant_docs in filtered_docs for doc in relevant_docs.metadata[\"connections\"]]\n",
    "        else:\n",
    "            # Obtain parent document IDs for retrieved chunks\n",
    "            doc_ids = [doc.metadata[\"id\"] for doc in filtered_docs]\n",
    "\n",
    "        # Obtain full documents based on IDs\n",
    "        filtered_docs = [DataFrameLoader(pd.DataFrame(self.dataframe[self.dataframe[\"id\"] == doc_id])).load()[0] for doc_id in set(doc_ids)]\n",
    "        return filtered_docs\n",
    "    \n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieved_docs: List[Document]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate response based on input query and raw context documents\n",
    "        \"\"\"\n",
    "        # Form context string based on retrieved documents\n",
    "        documents = [\"Title:\" + str(doc.metadata[\"title\"]) + \"\\n\" + str(doc.page_content) for doc in retrieved_docs]\n",
    "        context_str = \"\\n\\n\".join(documents)\n",
    "    \n",
    "        # Create prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "             (\"system\", \"\"\"\n",
    "                Find the answer to the user's QUESTION by using relevant keywords or phrases from the CONTEXT above. \n",
    "                Keep the answer extremely short. \n",
    "                You are not allowed to include any of your own words. You are only allowed to use words from the CONTEXT. \n",
    "              \"\"\"),\n",
    "            (\"human\", \"### CONTEXT\\n{context}\\n### QUESTION\\n{question}\")\n",
    "        ])\n",
    "\n",
    "        # Construct the LLM chain \n",
    "        chain = prompt | self.client\n",
    "        response = chain.invoke({\"question\": question, \"context\": context_str})\n",
    "\n",
    "        return response.content\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Before we start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two evaluation metrics are utilized:\n",
    "\n",
    "- BLEU score: measure the n-gram precision between a candidate text with a list of references. This should give us a rough idea of the quality of our generated answers compared to the ground truth answers.\n",
    "- Recall score: checks whether the ground truth context chunk has been retrieved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(ref_list: str, cand: str):\n",
    "    \"\"\"\n",
    "    BLEU score\n",
    "    \"\"\"\n",
    "    smoothing_func = SmoothingFunction().method1\n",
    "\n",
    "    # Clean reference and candidate text\n",
    "    reference = [word_tokenize(re.sub(r'[^\\w\\s]','', ref.lower())) for ref in ref_list]\n",
    "    candidate = word_tokenize(re.sub(r'[^\\w\\s]','', cand.lower()))\n",
    "\n",
    "    weight_configs = (1, 0, 0, 0)\n",
    "\n",
    "    bleu_score = sentence_bleu(\n",
    "        references=reference, \n",
    "        hypothesis=candidate, \n",
    "        weights=weight_configs, \n",
    "        smoothing_function=smoothing_func\n",
    "    )\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def compute_recall_score(ground_truth_context: str, retrieved_context: List[Document]):\n",
    "    \"\"\"\n",
    "    Whether the ground truth context document is retrieved \n",
    "    \"\"\"\n",
    "    retrieved_context_str = [doc.page_content for doc in retrieved_context]\n",
    "    return 1 if ground_truth_context in retrieved_context_str else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates an evaluation loop, which iterates through each row in the dataset to generate the results and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    agent: RAGAgent, \n",
    "    test_set: Dataset, \n",
    "    top_k: int, \n",
    "    q_top_k: int,\n",
    "    distance_threshold: float,\n",
    "    q_distance_threshold: float\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    # Loop through each row in the test set\n",
    "    for i in range(len(test_set)):\n",
    "        id = test_set[i][\"id\"]\n",
    "        question = test_set[i][\"question\"]\n",
    "        ground_truth_answer = test_set[i][\"answers\"][\"text\"]\n",
    "        ground_truth_context = test_set[i][\"context\"]\n",
    "\n",
    "        # Retrieve relevant chunks based on query\n",
    "        relevant_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=top_k, \n",
    "            distance_threshold=distance_threshold,\n",
    "            retrieve_questions=False\n",
    "        )\n",
    "\n",
    "        # Retrieve relevant chunks using question-based retrieval\n",
    "        relevant_question_chunks = agent.retrieve_context(\n",
    "            query=question, \n",
    "            top_k=q_top_k, \n",
    "            distance_threshold=q_distance_threshold,\n",
    "            retrieve_questions=True\n",
    "        )\n",
    "\n",
    "        # Concatenate the retrieved documents\n",
    "        context = relevant_question_chunks + relevant_chunks\n",
    "        # Invoke the generator chain\n",
    "        response = agent.generate_response(question, context)\n",
    "\n",
    "        # Compute metrics \n",
    "        bleu_score = compute_bleu_score(ground_truth_answer, response)\n",
    "        recall_score = compute_recall_score(ground_truth_context, context)\n",
    "\n",
    "        # Collect results\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"context\": [chunk.page_content for chunk in relevant_chunks],\n",
    "            \"q_context\": [chunk.page_content for chunk in relevant_question_chunks],\n",
    "            \"ground_truth_context\": ground_truth_context,\n",
    "            \"bleu_score\": bleu_score,\n",
    "            \"recall_score\": recall_score,\n",
    "            \"id\": id\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo #1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we evaluate the agent before and after the *corrective loop*. This could be useful to check whether Q-RAG can really improve the retrieval performance.\n",
    "\n",
    "#### Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive RAG Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initiate our baseline RAG Agent with access to the chunk database and the (empty) question database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation(\n",
    "    agent=rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=1,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-0-demo-1.json\", \"w\") as file:\n",
    "    file.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7147777497184254"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute average BLEU score\n",
    "bleu_scores = [result[\"bleu_score\"] for result in results] \n",
    "np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute average recall score\n",
    "recall_scores = [result[\"recall_score\"] for result in results] \n",
    "np.mean(recall_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-RAG Corrective Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to perform the corrective loop. This requires an external source to provide the ground truth answers. In practical scenarios, we can have human evaluators going through poor responses (e.g., responses reported by users, responses with low metric values) to correct them.\n",
    "\n",
    "We start by first create a copy of the initial question database (currently empty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the new question vectorstore\n",
    "new_question_vectorstore_db = deepcopy(question_vectorstore_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate through the results to detect poor answers, which are defined as BLEU < 0.6. For each of these instances, we are going to utilize the ground truth answer to retrieve the most relevant chunks. These chunks will then be put into the `connections` attribute of the corresponding questions. These questions documents are then appended to the question database. \n",
    "\n",
    "In the code, one may be able to observe that we are utilizing both the original question and the ground truth answer to retrieve relevant chunks here (Line 9). This is simply because SQuAD is a short-form QnA dataset, meaning that the expected answers are extremely short and concise, composing of just one or two keywords. Therefore, using the answer alone for context retrieval would be naive. As such, it is a reasonable option to concatenate the original question and the ground truth answer together for this task. In most cases, inlcluding just a few keywords from the corrected answers is enough to significantly improve the retrieval results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the BLEU scores\n",
    "for i, score in enumerate(bleu_scores):\n",
    "    # If performance is poor (BLEU < 0.6)\n",
    "    if score < 0.6:\n",
    "        id = dataset[i][\"id\"]\n",
    "        question = dataset[i][\"question\"]\n",
    "        ground_truth_answer_list = sorted(dataset[i][\"answers\"][\"text\"], key=len)\n",
    "        ground_truth_answer = ground_truth_answer_list[-1]\n",
    "\n",
    "        # Retrieve \"ground truth\" chunks using ground truth answers (and original question)\n",
    "        relevant_chunk_list = rag_agent.retrieve_context(\n",
    "            query=question + \" \" + ground_truth_answer,\n",
    "            top_k=2,\n",
    "            distance_threshold=1.5\n",
    "        )\n",
    "\n",
    "        # Obtain parent document IDs \n",
    "        relevant_chunk_id_list = [chunk.metadata[\"id\"] for chunk in relevant_chunk_list]\n",
    "\n",
    "        # Construct new \"model question\" with connections to \"ground truth\" chunks\n",
    "        question_document = Document(\n",
    "            page_content=question,\n",
    "            metadata={\n",
    "                \"id\": id,\n",
    "                \"type\": \"question\", \n",
    "                \"connections\": relevant_chunk_id_list\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add new questions to the vectorstore\n",
    "        new_question_vectorstore_db.add_documents([question_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorstore locally\n",
    "new_question_vectorstore_db.save_local(f\"{FAISS_PATH}/question-vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the vectorstore from local\n",
    "new_question_vectorstore_db = FAISS.load_local(\n",
    "    folder_path=f\"{FAISS_PATH}/question-vectorstore\", \n",
    "    embeddings=embedding_model, \n",
    "    normalize_L2=True,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-RAG Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new agent with access to the newly indexed question database. This would be our Q-RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rag_agent = RAGAgent(\n",
    "    client=llm_agent,\n",
    "    chunk_retriever=chunks_vectorstore_db,\n",
    "    question_retriever=new_question_vectorstore_db,\n",
    "    dataframe=document_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-RAG agent then undergoes the same evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-1-demo-1.json\", \"w\") as file:\n",
    "    file.write(json.dumps(new_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225012975337309"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute average BLEU score\n",
    "new_bleu_scores = [result[\"bleu_score\"] for result in new_results] \n",
    "np.mean(new_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute average recall score\n",
    "new_recall_scores = [result[\"recall_score\"] for result in new_results] \n",
    "np.mean(new_recall_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the improvement in the average BLEU score in small, it is clear to observe that there has been a large improvement in context retrieval score. This implies that Q-RAG corrective ability performs quite well in retrieving chunks with ground truth answer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo #2\n",
    "\n",
    "The previous demo utilized the same question ingested in the database to evaluate Q-RAG, which may not be fair since the question in the test set can be directly matched with itself in the database. Therefore, in this demo, we will develop a new question dataset that are the rephrased version of the questions in the original dataset. This allows us to observe how Q-RAG performs in a more realistic scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Evaluation Dataset\n",
    "\n",
    "Here, we would like to select only the \"complex questions\" - ones with BLEU < 0.6 when evaluated using vanilla RAG - to construct our new dataset. These are the same questions that has been ingested in the question vector database ealier. By separating these questions out, it is possible to focus our evaluation into the question-based retrieval capability of the Q-RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 344\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain complex questions (that lead to poor performance) from the original dataset\n",
    "complex_question_dataset = Dataset.from_list([row for index, row in enumerate(dataset) if bleu_scores[index] < 0.6])\n",
    "complex_question_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function utilizes the LLM to rephrase the questions in the dataset. Here, we avoid providing the ground truth answers to the LLM because we do not want the rephrased questions to be too semantically similar to the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_question(row: str):\n",
    "    # Construct prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "            You are provided with a QUESTION. \n",
    "            Your task is to transform the QUESTION into a REPHRASED QUESTION by changing only the wording. \n",
    "            Make sure to leave enough information in the REPHRASED QUESTION so that the the answer to the do not change.\n",
    "        \"\"\"),\n",
    "        (\"user\", \"QUESTION: {question}\\nREPHRASED QUESTION:\")\n",
    "    ])\n",
    "\n",
    "    # Construct LLM chain\n",
    "    chain = prompt | llm_agent\n",
    "    rephrased_question = chain.invoke({\"question\": row[\"question\"]}).content\n",
    "\n",
    "    # Replace existing question with the rephrased version \n",
    "    row[\"question\"] = re.sub(\"rephrased question:\", \"\", rephrased_question.lower()).strip()   \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function rephrase_question at 0x000001F3D3712D40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1d14d1ba874ebd811eb4b1e8f96e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rephrased_dataset = complex_question_dataset.map(rephrase_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The evaluation process takes place on 3 different scenarios:\n",
    "\n",
    "- Vanilla RAG on complex question dataset (baseline for comparison).\n",
    "- Q-RAG on complex question dataset.\n",
    "- Q-RAG on rephrased complex question dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vanilla RAG on complex questions (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results_complex = evaluation(\n",
    "    agent=rag_agent,\n",
    "    test_set=complex_question_dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-0-demo-2.json\", \"w\") as file:\n",
    "    file.write(json.dumps(rag_results_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2501785283062391"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average BLEU score\n",
    "np.mean([result[\"bleu_score\"] for result in rag_results_complex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7383720930232558"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average recall score\n",
    "np.mean([result[\"recall_score\"] for result in rag_results_complex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q-RAG on complex questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrag_results_complex = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=complex_question_dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-1-demo-2.json\", \"w\") as file:\n",
    "    file.write(json.dumps(qrag_results_complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39212412498484117"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average BLEU score\n",
    "np.mean([result[\"bleu_score\"] for result in qrag_results_complex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average recall score\n",
    "np.mean([result[\"recall_score\"] for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q-RAG on rephrased complex questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrag_results_rephrased = evaluation(\n",
    "    agent=new_rag_agent,\n",
    "    test_set=rephrased_dataset,\n",
    "    top_k=5,\n",
    "    q_top_k=2,\n",
    "    distance_threshold=1.5,\n",
    "    q_distance_threshold=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULTS_PATH}/run-2-demo-2.json\", \"w\") as file:\n",
    "    file.write(json.dumps(qrag_results_rephrased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39325279700091936"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average BLEU score\n",
    "np.mean([result[\"bleu_score\"] for result in qrag_results_rephrased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8924418604651163"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average recall score\n",
    "np.mean([result[\"recall_score\"] for result in qrag_results_rephrased])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how the example question are rephrased, the results may vary. In most cases, it can be observed that:\n",
    "- There has been a considerable improvement in recall score moving from vanilla RAG to Q-RAG. \n",
    "- The Q-RAG agent was able to obtain a highly similar performance (to the original complex question dataset) when performing on the rephrased question dataset. This indicates that Q-RAG's question-based retrieval mechanism is highly comparable when encountering new similar questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
